<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://minsuukim.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://minsuukim.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2022-10-09T01:59:51+09:00</updated><id>https://minsuukim.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Finite Element Networks</title><link href="https://minsuukim.github.io/2022/06/06/finite-elements.html" rel="alternate" type="text/html" title="Finite Element Networks"/><published>2022-06-06T02:39:00+09:00</published><updated>2022-06-06T02:39:00+09:00</updated><id>https://minsuukim.github.io/2022/06/06/finite-elements</id><content type="html" xml:base="https://minsuukim.github.io/2022/06/06/finite-elements.html"><![CDATA[<h1 id="learning-the-dynamics-of-physical-systems-from-sparse-observations-with-finite-element-networks">Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks</h1> <p>We will present a blog post on <a href="https://arxiv.org/abs/2203.08852"><em>“Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks”</em></a> from Marten Lienen and Stephan Günnemann [1], which has been accepted as a Spotlight presentation in <a href="https://iclr.cc/">ICLR 2022</a>. This was originally a course review for <a href="https://dsail.gitbook.io/isyse-review/paper-review/2022-spring-paper-review">KSE 527</a> at KAIST.</p> <h2 id="1-problem-definition"><strong>1. Problem Definition</strong></h2> <p>We will firstly introduce the problem in a brief, yet somewhat lengthy, necessary background about differential equations and the finite element method that constitutes a backbone of the paper.</p> <h3 id="modeling-complex-systems-with-differential-equations">Modeling Complex Systems with Differential Equations</h3> <p>Differential Equations are regarded by many as the <em>language of nature</em>. Many complex systems can be modeled by describing each single variable as a relation with others in both <em>space</em> and <em>time</em>: Partial Differential Equations (PDEs) describe such processes. A quite general formulation can be written as following:</p> \[\partial_t u = F (t, x, u, \partial_x u, \partial_{x^2} u, \dots)\] <p>where \(u\) is a solution of the equation and \(F\) are the <em>dynamics</em> which can be a function of time, space, \(u\) itself and its derivatives. PDEs are generally either very expensive to compute if not intractable altogether. For these reason, multiple algorithms have been developed over the centuries to try and solve this extremely complex endeavor. In particular, computers are very capable of handling <em>discretized</em> data, in the form of digital bits instead of their continuous, analog counterparts. Can we apply some algorithm which is well suitable to computers?</p> <h3 id="the-finite-element-method">The Finite Element Method</h3> <p>The Finite Element Method (FEM) is a way to <em>divide and conquer</em> the realm of PDEs.</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/fem-example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/fem-example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/fem-example-1400.webp"/> <img src="/assets/img/blogs/fen/fem-example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1.<i> An example of Finite Element Method (FEM) applied to a magnetical shield. </i> </div> </div> <p>In particular, the domain with set of points \(\mathcal{X}\) is divided into a set of simplices (i.e., \(n\) -dimensional triangles) which is called <em>triangulation</em>. Triangulations, such as the Delaunay triangulation, are also referred to as <em>meshes</em> and are frequently used in many other areas such as movie CGI, gaming and most 3D graphics. This can be seen on the left of Figure 1. Then, operations are performed on this discretized domain to obtain a solution, as shown on the right of Figure 1.</p> <h4 id="basis-functions">Basis Functions</h4> <p>In general, the solution \(u\) would lie in an infinite-dimensional space \(\mathcal{U}\). What if, however, we cannot have infinite dimensions? Then, we need to approximate \(u\) with a finite-dimensional subspace \(\mathcal{\tilde{U}}\). To do so we employ <em>basis functions</em> \(\varphi\), which map points from \(\mathcal{U}\) to \(\mathcal{\tilde{U}}\). The simplest choice, which the authors use, is the P1 piecewise linear functions which map</p> \[\varphi^{(j)} (x^{(i)}) = \begin{cases} 1 &amp; \text{if }x^{(i)} = x^{(j)}\\ 0 &amp; \text{otherwise} \end{cases} \quad \forall x^{(i)} \in \mathcal{X}.\] <p>that is basically to simply map each point in \(\mathcal{U}\) to the same values in \(\mathcal{\tilde{U}}\) as in Figure 2 left.</p> <p>Moreover, another property of expanding \(u \in \mathcal{\tilde{U}}\) is that the following holds:</p> \[u(x^{(i)}) = \sum_{j=1}^N c_j \varphi^{(j)}(x^{(i)}) = c_i\] <p>i.e., the value of \(u\) at the \(i\)-th node is just its \(i\)-th coefficient.</p> <h4 id="galerkin-method">Galerkin Method</h4> <p>The piecewise linear approximation above is not differentiable everywhere. However, we can constrain the residual \(R\), i.e. the difference between \(\partial_t u\) and \(F\) to be orthogonal to the approximation space:</p> \[\langle R(u), \varphi^{(i)} \rangle_\Omega = 0 \quad \forall i \in 1, \dots, N\] <p>where \(\Omega\) represents the spatial domain. In simpler terms, we are asking for the <em>best possible</em> approximation of the equations. Given this, we can now reconstruct the equation as following</p> \[\langle \partial_t u, \varphi^{(i)}\rangle_\Omega = \langle F (t, x, u, \partial_x u, \partial_{x^2} u, \dots), \varphi^{(i)}\rangle_\Omega, \quad \forall i \in 1, \dots, N\] <p>By stacking the equations above we obtain the following linear system \(A \partial_t c = m\)</p> <p>where \(A\) with \(A_{ij} = \langle \varphi^{(i)}, \varphi^{(j)} \rangle_\Omega\) is the so-called mass matrix, \(c\) is the vector of basis coefficients of \(u\) and \(m\) with \(m_i = \langle F(t, x, u, \dots), \varphi^{(i)} \rangle_\Omega\) captures the effects of dynamics \(F\).</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/basis-function-choice-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/basis-function-choice-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/basis-function-choice-1400.webp"/> <img src="/assets/img/blogs/fen/basis-function-choice.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2.<i> Solving a PDE with the Galerkin method and method of lines consists of three steps. </i> </div> </div> <h4 id="method-of-lines">Method of Lines</h4> <p>If we can evaluate the right hand side \(m\), then the equations are easily solvable with time derivatives. In particular, we can consider a <em>stacked</em> version of multiple scalar fields instead of vector ones as</p> \[\begin{equation} A \partial_t C = M \end{equation} \tag{1}\] <p>where \(M\) are \(m\)-dimensional matrices due to \(m\)-scalar fields. In practice, we have transformed a PDE into a matrix ODE (ordinary differential equation) by discretizing in space; we managed to obtain a much simpler way of solving our problem by only needing to <em>integrate</em> over time: a much simpler task!</p> <h2 id="2-motivation"><strong>2. Motivation</strong></h2> <p>PDEs are the <em>language of nature</em> and as such they are incredibly important for the scientific community. However, many hand-crafted models either take too long to compute solutions or do not have enough expressibility. Therefore, it is necessary to include at least partial, <em>data-driven</em> terms that can learn from past experiences.</p> <p>Machine and Deep Learning have proven incredibly powerful tools for solving real-world complex phenomena: they can accelerate simulations by orders of magnitude enabling faster predictions, design and control and even describe previously unknown dynamics which cannot be derived by equations.</p> <p>There are mainly two lines of research in the area of PDEs and Deep Learning: either constraining PDE solution learning with a cost function, or learning directly from data to obtain a simulator via inductive biases.</p> <p>In this work, the authors follow the second path and derive a model which sprouts from research on numerical methods for differential equations and can incorporate knowledge of dynamics (such as transport terms).</p> <h2 id="3-method-finite-element-networks"><strong>3. Method: Finite Element Networks</strong></h2> <h4 id="from-the-finite-element-equation-to-learnable-models">From the Finite Element Equation to Learnable Models</h4> <p>We would like to find solutions to a PDE process via a data-driven simulator. Given the Finite Element <a href="#method-of-lines">Equation 1</a>, we can rewrite its terms as following:</p> <p>\(A \partial_t Y^{(t)} = M\) where \(A\) is the mass matrix and \(Y\) is the feature matrix - in other words, this part represents the <em>feature update</em> in time that we need to obtain the dynamics evolution in time. The problem at inference time then becomes:</p> <ol> <li>Evaluate matrix \(A\) and inverting it</li> <li>Evaluating matrix \(M\)</li> </ol> <p>We can readily obtain \(A\) by <em>mass lumping</em> [2] which allows for a good performance of the matrix inversion necessary to obtain \(\partial_t Y^{(t)}\). The right-hand term describing the <em>dynamics</em> as we have seen before requires an evaluation of the contribution of dynamics of adjacent cells:</p> \[M_{ik} = \langle F(t, x, u, \dots)_k, \varphi^{(i)} \rangle_\Omega = \sum_{\Delta} \langle F(t, x, u, \dots)_k, \varphi^{(i)} \rangle_{CH(\Delta)}\] <p>where $\Delta$ is the set of mesh cells adjacent to \(x^{(i)}\) and \(CH(\Delta)\) is the <em>convex hull</em> (i.e., smallest convex set of $\Delta$ that contains it). As we can see, evaluating $M$ (which we call the <strong>message matrix</strong>) is actually the same as operating <strong>message passing</strong> between adjacent cells. This means that we can represent these dynamics with a Message Passing Neural Network!</p> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/FEN-catchy-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/FEN-catchy-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/FEN-catchy-1400.webp"/> <img src="/assets/img/blogs/fen/FEN-catchy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure .<i> Finite Element Networks: we can evaluate dynamics by message passing over adjacent cells and integrating this value to obtain the future values. </i> </div> </div> <p>Moreover, by factoring the inner product on the right side of the previous equation as</p> \[\langle F(t, x, u, \dots)_k, \varphi^{(i)} \rangle_{CH(\Delta)} = F_{\Delta, k}^{(i)} \cdot \langle 1, \varphi^{(i)} \rangle_{CH(\Delta)} \tag{2}\] <p>we can avoid numerical instabilities and learn spatial derivatives as well. This means that we can learn a model \(f_\theta \approx F_\Delta^{(i)}\)!</p> <h4 id="model">Model</h4> <p>We have seen from <a href="#from-the-finite-element-equation-to-learnable-models">Equation 2</a> that we can learn a model by performing message passing over adjacent cells. In particular, the learned model \(f_\theta\) can be written as:</p> \[f_{\theta, \Delta}^{(t, i)} = f_\theta \left( t, \mu_\Delta, x_\Delta, y_\Delta^{(t)} \right)^{(i)} \approx F_\Delta^{(i)}\] <p>where \(\mu_\Delta\) is the center of cell \(\Delta\), \(x_\Delta\) are the coordinates of cell vertices w.r.t. \(\mu\) and \(y^{(t)}\) are the features at the vertices at time \(t\). We have written the equations for a single message passing step, which is the update at each single time step. To obtain a whole trajectory, we need to solve the associated ODE given an initial condition \(y^{(t_0)}\) and times \(t \in 0, 1, \dots, T\):</p> \[y^{(t_0, t_1, \dots, t_N)} = \text{ODESolve}(y^{(t_0)}, \partial_t y , t)\] <p>This ODE can be solved in a variety of ways. In particular, the authors employ the \(\tt dopri5\) adaptive-step solver, i.e., an solver that iterative computes the solution by calling the function multiple times. We resulting model <strong>FEN</strong>: Finite Element Network.</p> <h4 id="modeling-the-transport-term">Modeling the Transport Term</h4> <p>What if we have some extra knowledge about the domain? For example, an assumption on the dynamics \(F\) could be that our solution would be at least in part governed by a <em>convective component</em> (i.e. describing fluid motion):</p> \[F(t, x, u, \dots)_k= \underbrace{ - \nabla \cdot (v^{(k)} (t, x, u, \dots) u_k)}_{convection~term} + \underbrace{F'(t, x, u, \dots)_k}_{remainder~dynamics}\] <p>where \(v\) is the divergence-free velocity term.</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/airfoils-example-velocity-field-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/airfoils-example-velocity-field-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/airfoils-example-velocity-field-1400.webp"/> <img src="/assets/img/blogs/fen/airfoils-example-velocity-field.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> Example flow field around airfoils present convective components.</i> </div> </div> <p>We can model \(F'\) as in the previous case while we can model the convection term by message passing with the following network \(g_\vartheta\):</p> \[f^v_{msg} (\Delta)_{x^{(i)}} = \sum_{x^{(j)} \in \Delta} y^{(t, j)} \odot \left( g_{\vartheta,\Delta}^{(t, i)} \cdot \langle \nabla \varphi^{(j)}, \varphi^{(i)} \rangle_{CH(\Delta)} \right)\] <p>The final model, which is called <strong>T-FEN</strong>: Trasport-FEN, is the sum of the message passing of the above convection term and \(F'\) and is thus designed to capture both a velocity field and remainder dynamics.</p> <h2 id="4-experiments"><strong>4. Experiments</strong></h2> <h3 id="baselines">Baselines</h3> <p>The authors consider the following baselines:</p> <ul> <li><strong>Graph WaveNet (GWN)</strong>: combines temporal and graph convolutions [3]</li> <li><strong>Physics-aware Difference Graph Network (PA-DGN)</strong>: estimates spatial derivatives as additional features for a recurrent graph network [4]</li> <li><strong>Continuous-time MPNN (CT-MPNN)</strong> model in uses a general MPNN to learn the continuous-time dynamics of the data [5]</li> </ul> <h3 id="datasets">Datasets</h3> <h4 id="cylinder-flow">Cylinder Flow</h4> <p>The following dataset consists of simulated flow fields around a cylinder as collected by [6]. The dataset includes velocities and pressures along with marked mesh cells representing boundary walls, inlets, outlets and cylindrical obstacles of varying sizes. The sequences contain \(600\) frames and are divided in \(1000-100-100\) for train, validation and test. The time resolution \(\Delta t\) is of \(0.01~s\).</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/cylinder-flow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/cylinder-flow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/cylinder-flow-1400.webp"/> <img src="/assets/img/blogs/fen/cylinder-flow.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> Example flow field around airfoils present convective components.</i> </div> </div> <h4 id="black-sea">Black Sea</h4> <p>This dataset is composed data on daily mean sea surface temperature and water velocities on the Black Sea over several years. The training data is made of frames from 2012 to 2017, validation is on frames from 2018 and testing is done with frames from the year 2019. The time resolution \(\Delta t\) is of 1 day.</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/black-sea-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/black-sea-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/black-sea-1400.webp"/> <img src="/assets/img/blogs/fen/black-sea.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> Learned flow fields of water velocities on the Black Sea dataset: T-FEN recognized the relationships between features. </i> </div> </div> <h4 id="scalarflow">ScalarFlow</h4> <p>This dataset consists of 3D reconstructions generated by multiple camera views of rising hot smoke plumes in a real environment. The sequences contain \(150\) frames and are divided in \(64-20-20\) for train, validation and test. The time resolution \(\Delta t\) is of \(0.0167~s\) (recording was done at 60 fps)[7].</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/scalarflow-comparison-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/scalarflow-comparison-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/scalarflow-comparison-1400.webp"/> <img src="/assets/img/blogs/fen/scalarflow-comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> Long-range extrapolations on the ScalarFlow dataset (60 time steps). FEN models perform better than the strongest baseline by also better modeling of sources and sinks. </i> </div> </div> <h3 id="model-parameters">Model Parameters</h3> <p>Both networks \(f_\theta\) and \(g_\vartheta\) are <em>multi-layer perceptrons</em> (MLPs) with $\tt tanh$ nonlinearities. The number of parameters of each network was kept similar between FEN and T-FEN models and lower than baseline to demonstrate their capabilities.</p> <h3 id="results">Results</h3> <h4 id="multi-step-forecasting">Multi-step Forecasting</h4> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/table-experiments-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/table-experiments-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/table-experiments-1400.webp"/> <img src="/assets/img/blogs/fen/table-experiments.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> Multi-step Forecasting. </i> </div> </div> <p>This experiments aims at predicting \(10\) steps in the future. We can see that FEN models either outperform or achieve similar, competitive results with the baselines.</p> <h4 id="super-resolution">Super-resolution</h4> <div style="width:60%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/plot-mae-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/plot-mae-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/plot-mae-1400.webp"/> <img src="/assets/img/blogs/fen/plot-mae.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> Errors with super-resolution in the number of nodes. </i> </div> </div> <p>This experiments aims at predicting \(10\) steps in the future as before but with varying number of nodes, i.e. more nodes than those seen during training. FEN models outperform baselines in super-resolution: T-FEN models always perform better than FEN counterparts since they can better represent transport terms.</p> <h4 id="extrapolation">Extrapolation</h4> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/scalarflow-extrapolation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/scalarflow-extrapolation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/scalarflow-extrapolation-1400.webp"/> <img src="/assets/img/blogs/fen/scalarflow-extrapolation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> Extrapolation over 60 steps. </i> </div> </div> <p>This experiments aims at predicting \(60\) steps in the future with models trained on \(10\) steps. FEN models outperform baselines since they can correctly represent sources and sinks.</p> <h4 id="interpretability">Interpretability</h4> <div style="width:50%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/fen/freeform-transport-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/fen/freeform-transport-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/fen/freeform-transport-1400.webp"/> <img src="/assets/img/blogs/fen/freeform-transport.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <i> T-FEN model providing an interpretable splitting between free-form and transport term. </i> </div> </div> <p>This experiments aims at providing interpretability and a justification for the T-FEN model. Plotting the free-form and transport term separately provides an interesting view into the learning process which is interpretable - the transport represents the differences in flow field.</p> <h2 id="5-conclusion"><strong>5. Conclusion</strong></h2> <p>We have reviewed <em>Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks</em>, a novel graph paradigm for learning dynamics on graphs based on inductive biases from differential equations. The authors provided a detailed analysis of the method from the ground up - starting from the theory of Finite Element analysis - and then devised two main models variations. While the first one learns directly the solution derivative in time of the physical system, the second separates learning with a <em>transport term</em> which is shown to improve learning under many conditions. The experiments were conducted in one syntethic and two real-world high-dimensional datasets. Results demonstrated that the proposed models either perform competitively or outperform state-of-the-art baselines. This work represents and important contribution to the scientific machine learning community by tightly integrating the theory of Finite Element Method and Graph Neural Networks.</p> <h4 id="limitations">Limitations</h4> <p>The proposed model uses a simple basis - namely, linear piecewise basis function. If higher order derivatives were used, such as second order, these basis functions would evaluate to \(0\), which is thus a current limitation of the model. Another limitation is the number of function evaluations: it is shown that the models can take more than 300 evaluations, while other non-continuous models may require just one. This is due to the adaptive ODE solvers used. Although the model can theoretically describe continuous dynamics, this practically makes it way slower than <em>one-step-prediction</em> counterparts that do not need to evaluate an ODE.</p> <hr/> <h2 id="author-information"><strong>Author Information</strong></h2> <p><strong>Federico Berto</strong> <a href="https://fedebotu.github.io/">Personal Website</a></p> <p>Affiliation: KAIST, Industrial &amp; Systems Engineering Department</p> <p>MSc students at <a href="http://silab.kaist.ac.kr/">SILAB</a></p> <p>Member of the open research group <a href="https://github.com/DiffEqML">DiffEqML</a></p> <h2 id="6-reference--additional-materials"><strong>6. Reference &amp; Additional materials</strong></h2> <h3 id="github-implementation">Github Implementation</h3> <p><a href="https://github.com/martenlienen/finite-element-networks">https://github.com/martenlienen/finite-element-networks</a>.</p> <h3 id="references">References</h3> <p>[1] Lienen, Marten, and Stephan Günnemann. “Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks.” ICLR (2022).</p> <p>[2] Lapidus, Leon, and George F. Pinder. “Numerical solution of partial differential equations in science and engineering”. John Wiley &amp; Sons (2011).</p> <p>[3] Wu, Zonghan, et al. “Graph wavenet for deep spatial-temporal graph modeling.” CoRR (2019).</p> <p>[4] Seo, Sungyong, Chuizheng Meng, and Yan Liu. “Physics-aware difference graph networks for sparsely-observed dynamics.” ICLR (2019).</p> <p>[5] Iakovlev, Valerii, Markus Heinonen, and Harri Lähdesmäki. “Learning continuous-time PDEs from sparse data with graph neural networks.” ICLR (2021).</p> <p>[6] Pfaff, Tobias, et al. “Learning mesh-based simulation with graph networks. ICLR (2021).</p> <p>[7] Eckert, Marie-Lena, Kiwon Um, and Nils Thuerey. “ScalarFlow: a large-scale volumetric data set of real-world scalar transport flows for computer animation and machine learning.” ACM Transactions on Graphics (TOG) (2019)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A review on the paper "Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks" (ICLR 2022)]]></summary></entry><entry><title type="html">Equivariant Subgraph Aggregation Networks</title><link href="https://minsuukim.github.io/2022/04/16/esan.html" rel="alternate" type="text/html" title="Equivariant Subgraph Aggregation Networks"/><published>2022-04-16T02:39:00+09:00</published><updated>2022-04-16T02:39:00+09:00</updated><id>https://minsuukim.github.io/2022/04/16/esan</id><content type="html" xml:base="https://minsuukim.github.io/2022/04/16/esan.html"><![CDATA[<h1 id="equivariant-subgraph-aggregation-networks">Equivariant Subgraph Aggregation Networks</h1> <p>We will present a blog post on <a href="https://arxiv.org/abs/2110.02910"><em>“Equivariant Subgraph Aggregation Networks”</em></a> from Beatrice Bevilacqua et al. [1], which has been accepted as a Spotlight presentation in <a href="https://iclr.cc/">ICLR 2022</a>. This was originally a course review for <a href="https://dsail.gitbook.io/isyse-review/paper-review/2022-spring-paper-review">KSE 527</a> at KAIST.</p> <h2 id="1-problem-definition"><strong>1. Problem Definition</strong></h2> <h3 id="message-passing-neural-networks-and-their-drawbacks">Message Passing Neural Networks and their Drawbacks</h3> <p>The commonly used Message-passing Graph Neural Networks (MPNNs) consist of several layers which perform node-wise aggregation of information from neighbour nodes.</p> <p>There are two main important characteristics which make these graph neural networks appealing:</p> <ul> <li><strong>Locality</strong>: computations require only the immediate neighbours of a node. Unlike Convolutional Neural Networks (CNNs), which require structured data, MPNNs are able to capture complex relationships on unstructured graphs, which makes them suitable for a number of different tasks that cannot be solved by the classical convolutions of CNNs.</li> <li><strong>Linear complexity</strong>: MPNNs benefit from linear in the number of edges: this means that they can be easily <em>scalable</em> to large numbers of nodes and edges. Roughly speaking, this <em>linear complexity</em> property means that doubling the number of nodes in our graph will double the requirement in computational resources.</li> </ul> <p>However, MPNNs suffer from a limitation on their expressive power. In particular, it has been demonstrated that they are at most as expressive as the <a href="https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49">Weisfeiler-Lehman graph isomorphism test</a>, a classical method that can be used to determine if the structure of two graphs is <em>equivalent</em> (i.d. <em>iso</em>: same, <em>morphic</em>: shape). So, what is this Weisfeiler-Lehman test?</p> <h3 id="a-brief-introduction-to-the-weisfeiler-lehman-test">A Brief Introduction to the Weisfeiler-Lehman Test</h3> <p>The Weisfeiler-Lehman (WL) graph isomorphism test is a <em>necessary</em> but <em>insufficient</em> condition to determine if two graphs have the same structure. In particular the WL-test is an efficient <em>heuristics</em> that can tell in polynomial time if two graphs are not isomorphic:</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/wl_test-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/wl_test-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/wl_test-1400.webp"/> <img src="/assets/img/blogs/esan/wl_test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>The algorithm works by <em>colouring</em> the graph nodes (i.d. assigning labels) and keeps assigning these colors by aggregating neighbors (we note that this aggregation is indeed a form of message passing!) and stops when the coloring is stable. When this happens, there are two possibilities:</p> <ul> <li>The colors are <strong>different</strong>: the two graphs are <strong>not</strong> isomorphic</li> <li>The colors are <strong>same</strong>: the two graphs <strong>may be</strong> (we are not sure!) isomorphic</li> </ul> <p>This is the reason why it is an <em>insufficient</em> condition for isomorphism. Let’s look at this example:</p> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/isomorphic_graphs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/isomorphic_graphs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/isomorphic_graphs-1400.webp"/> <img src="/assets/img/blogs/esan/isomorphic_graphs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>In this case, both of the graphs have the same number of nodes and edges and they are the same for the WL-test (same colors). However, they are clearly <strong>not isomorphic</strong>! This kind of structure occurs frequently in many graphs, such as molecular bonds.</p> <h3 id="extension-to-k-wl">Extension to <em>k</em>-WL</h3> <p>It has been shown that we can extend the WL-test to a higher order (<em>k</em>) version, namely the <em>k</em>-WL test. Except for 2-WL, it can be shown that (<em>k + 1</em>)-WL is strictly stronger than <em>k</em>-WL. However, there is no version for infinite dimensional WL, i.d. having a <em>general</em> and most importantly <em>sufficient</em> condition for isomorphism for <em>any</em> graph.</p> <h2 id="2-motivation"><strong>2. Motivation</strong></h2> <p>We have seen how MPNNs suffer from an <em>expressive power</em> drawback. While a number of approaches have been introduced in the literature to have more expressive GNNs, these have limitations such as:</p> <ul> <li>Poor generalization</li> <li>Computationally expensive</li> <li>Require deep domain knowledge</li> </ul> <p>What if we had a way a <em>general, inexpensive and domain-agnostic</em> way of performing augmenting GNN expressivity?</p> <h3 id="using-subgraphs-as-wl-tests">Using Subgraphs as WL-tests</h3> <p>A core idea of the paper is to divide a graph into subgraphs. We can use some <em>selection policies</em> (described later in the post) to obtain a <em>bag of subgraphs</em> from the original graph. Let’s see this example:</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/teaser-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/teaser-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/teaser-1400.webp"/> <img src="/assets/img/blogs/esan/teaser.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>As we can see on the left, there are WL-indistinguishable subgraphs: they are <strong>not-isomorphic</strong>, but they yield the <strong>same coloring</strong>. Now, if we split them into subgraphs (on the right) and run again the WL-test, we will notice that the produce coloring are indeed <strong>different</strong>: this implies that they are actually <strong><em>not isomorphic</em></strong>!</p> <h2 id="3-method"><strong>3. Method</strong></h2> <p>We will introduce the framework devised by the authors, <em>ESAN</em> (Equivariant Subgraph Aggregation Networks). The main idea behind ESAN is to represent the graph \(G\) as a <em>bag</em> (i.d. a multiset):</p> \[S_G=\{\{ G_1,\dots,G_m \}\}\] <p>of its subgraphs. Then, we can make predictions on a graph based on this subset: \(F(G):=F(S_G)\)</p> <p>There are two essential points to consider then: Two crucial questions pertain to this approach: (1) , and (2)</p> <ol> <li>How to define \(F(S_G)\), i.e. architecture should we use to process bags of graphs?</li> <li>How do we select \(S_G\), the graph subgraph selection policy?</li> </ol> <h3 id="bag-of-graphs-encoder-architecture">Bag-of-Graphs Encoder Architecture</h3> <h4 id="symmetry-groups">Symmetry groups</h4> <p>First of all, let’s start by describing how to represent a set of graphs (the so-called <em>bag-of-graphs</em>). We can summarize it in the following image:</p> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/symmetries-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/symmetries-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/symmetries-1400.webp"/> <img src="/assets/img/blogs/esan/symmetries.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>The graph is split into a bag of subgraphs where $\tau$ permutes the subgraphs in the set and $\sigma$ permutes the nodes in the subgraphs. In other words, this representation creates a <em>symmetry group</em>: by requiring the representation to permute the subgraphs and their nodes, we can obtain a neural architecture which is <strong>equivariant</strong> to this group.</p> <p>Equivariant means that no matter the order of transformations, the output will be always the same (on the left):</p> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/equivariance-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/equivariance-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/equivariance-1400.webp"/> <img src="/assets/img/blogs/esan/equivariance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>which is a desirable property in the realm of Graph Neural Networks - in the example above, it is easy to see how this is desirable for a classification CNN.</p> <h4 id="architecture-overview">Architecture Overview</h4> <p>Now, we can formulate the architecture of <strong>DSS-GNN</strong>:</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/arch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/arch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/arch-1400.webp"/> <img src="/assets/img/blogs/esan/arch.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>where DSS-GNN stands for the somewhat-lenghty “Deep Sets of Symmetric Objects - Graph Neural Network”; the symmetric objects are the bags of subgraphs that we obtained before. This architecture is composed of three main layers:</p> \[F_{\text{DSS-GNN}} = E_{\text{sets}}\circ R_{\text{subgraphs}} \circ E_{\text{subgraphs}}\] <p>Let’s decompose them one by one!</p> <ol> <li> <p><strong>Equivariant Feature Encoder</strong>: \(E_{\text{subgraphs}}\) is composed of several \(H\)-equivariant layers. Its purpose is to learn useful node features for all the nodes in all subgraphs. Each $H$-equivariant layer (on the right) processes bags of subgraphs accounting for their natural symmetry and it is composed by the following: \((L(\mathcal{A},\mathcal{X}))_i= L^1(A_i,X_i) + L^2\left(\textstyle\sum _{j=1}^m A_j,\textstyle\sum _{j=1}^m X_j\right)\) where, \(A_j, X_j\) are the adjacency and feature matrices of the \(j\)-th subgraph and \((L(\mathcal{A},\mathcal{X}))_i\) is the output of the layer on the $i$-th subgraph. So what are these \(L_1\) and \(L_2?\) These can be any type of GNN layer. While \(L_1\) encodes each subgraph <em>separately</em> , \(L_2\) <em>aggregates</em> information among the subgraphs (which is called <em>information sharing</em>). This is also common with the seminal DSS paper [2].</p> </li> <li> <p><strong>Subgraph Readout Layer</strong>: \(R_{\text{subgraphs}}\) given the output from the first step, this generates an invariant feature vector for each subgraph independently by aggregating the graph node and/or edge data. The modalities can change, but we can for instance aggregate with a mean operator.</p> </li> <li> <p><strong>Set Encoder</strong>: \(E_{\text{sets}}\) is is a universal set encoder, such as DeepSets [3] or PointNets [4]. This part aggregates the set of preprocessed subgraphs with <em>invariant-per-subgraph</em> features, so that we can obtain the final graph representation.</p> </li> </ol> <h3 id="subgraph-selection-policies">Subgraph Selection Policies</h3> <p>Selecting the subgraphs is of paramount importance: this determines how <em>expressive</em> our representation will be! The authors study four main policies:</p> <ol> <li><strong>Node-deleted policy</strong>: a graph is mapped to the set containing all subgraphs that can be obtained from the original graph by removing a single node</li> <li><strong>Edge-deleted policy</strong>: similar to above, but it is defined as the set of all subgraphs we obtain by removing a single edge.</li> <li><strong>EGO-networks policy</strong>: this policy maps each graph to a set of ego-networks of some specified depth, one for each node in the graph. An ego-network is, simply put, the network obtained by looking at the neighboorhood of a node only (where <em>ego</em> means “I”, meaning we focus on the perspective of one node). A \(k\)-Ego-network of a node is its \(k\)-hop neighbourhood with the induced connectivity).</li> <li><strong>EGO+ policy</strong>: We can also consider a variant of the ego-networks policy where the root node (a is <em>e</em> in the example below) holds an identifying feature (EGO+).</li> </ol> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/ego-network-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/ego-network-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/ego-network-1400.webp"/> <img src="/assets/img/blogs/esan/ego-network.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>However, obtaining the set of subgraphs from a large network can be overly expensive. That is why the authors propose a <strong>stochastic subsampling</strong> in which only a small subset of subgraphs is sampled to calculate the loss function.</p> <h3 id="theoretical-contributions">Theoretical Contributions</h3> <p>An important contribution of the authors is showing that their model ESAN is expressive by demonstrating it theoretically.</p> <p>In particular, the authors devise a new variant of the Weisfeiler-Lehman test dubbed “DSS-WL” with different architectural and subgraph selection policies. To sum up, an important theoretical conclusion is that the ESAN architecture can distinguish 3-WL equivalent graphs using only a WL graph encoder - which is, the standard MPNN. Moreover, this can enhance the expressive power of stronger architectures.</p> <h2 id="4-experiments"><strong>4. Experiments</strong></h2> <p>The experiments demonstrate strong performance of the DSS-GNN model. The authors also compare the case in which the <em>information sharing</em> layer \(L_2\) is set to \(0\). Base encoders and graph selection policies are reported in parentheses.</p> <h3 id="demonstrating-expressive-power-on-synthetic-datasets">Demonstrating Expressive Power on Synthetic Datasets</h3> <p>The authors use <em>EXP, CEXP</em> and <em>CS</em> which are designed so that any 1-WL GNN cannot do better than random guess, such as a vanilla MPNN.</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/synthetic-datasets-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/synthetic-datasets-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/synthetic-datasets-1400.webp"/> <img src="/assets/img/blogs/esan/synthetic-datasets.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>We notice how the variants of baselines with DSS-GNN manage to - perfectly - solve the tasks while 1-WL GNN basically perform random guesses.</p> <h3 id="tudatasets">TUDatasets</h3> <p>This is a benchmark set of real datasets for classification and regression of various type, such as user molecules (PROTEINS) and movies (IMDB).</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/tu_datasets-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/tu_datasets-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/tu_datasets-1400.webp"/> <img src="/assets/img/blogs/esan/tu_datasets.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>Results show that the approach achieves state of the art in one dataset (PTC), but it still retains comparable results to the SoTA (state-of-the-art).</p> <h3 id="ogb">OGB</h3> <p>These datasets comprise <em>OGBG-MOLHIV</em> and <em>OGBG-MOLTOX21</em>, which are dedicated to molecular property predictions.</p> <div style="width:60%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/ogb-dataset-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/ogb-dataset-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/ogb-dataset-1400.webp"/> <img src="/assets/img/blogs/esan/ogb-dataset.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>These results shows how ESAN achieve SoTA in both datasets with basically all of the selection policies.</p> <h3 id="zinc12k">Zinc12k</h3> <p><em>ZINC12K</em> is a larger scale molecular benchmark: here, the authors want to demonstrate how their method can scale up to much larger dimensions:</p> <div style="width:60%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/zink12k-dataset-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/zink12k-dataset-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/zink12k-dataset-1400.webp"/> <img src="/assets/img/blogs/esan/zink12k-dataset.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>As the table shows, ESAN does not achieve SoTA. <em>However</em>, it achieves SoTA among the <strong>domain-agnostic</strong> models: indeed, SoTA is achieved by a model employing strong inductive biases, which is somewhat an “unfair” comparison since the authors employ <em>ad-hoc</em> tricks to solve the problem. This shows how ESAN can expand on large scales by being both <em>powerful</em> and <em>domain-agnostic</em>.</p> <h2 id="5-conclusion"><strong>5. Conclusion</strong></h2> <p>We have reviewed <em>ESAN</em>: Equivariant Subgraph Aggregration Networks, a novel graph paradigm. The core idea of ESAN is to decompose a graph into bags of subgraphs via subgraph selection policies and process them according to symmetry groups. A further contribution is that the proposed model is provably more powerful than standard MPNNs in performing WL graph isomorphism tests. Experimental results reach state-of-the-art in certain tasks and, where they don’t, still remain very competitive by achieving close results while being domain-agnostic.</p> <h4 id="limitations">Limitations</h4> <p>The proposed model needs to select the bags of subgraphs and by processing a large amount of obtained subgraphs it is strictly more computationally expensive than standard MPNNs, which limit their applicability, although the authors include a subsampling policy to partly overcome this issue. A further limitation is that, while it is true that the authors provide mathematical proofs and code, their approach still does not achieve SoTA in many of the TUDatasets, a standard graph benchmarking problem, highlighting the fact that further studies should be done to prove that the proposed ESAN approach is strictly superior to competitors not only theoretically, but also <em>experimentally</em>.</p> <hr/> <h2 id="author-information"><strong>Author Information</strong></h2> <p><strong>Federico Berto</strong> <a href="https://fedebotu.github.io/">Personal Website</a></p> <p>Affiliation: KAIST, Industrial &amp; Systems Engineering Department</p> <p>MSc students at <a href="http://silab.kaist.ac.kr/">SILAB</a></p> <p>Member of the open research group <a href="https://github.com/DiffEqML">DiffEqML</a></p> <h2 id="6-reference--additional-materials"><strong>6. Reference &amp; Additional materials</strong></h2> <h4 id="github-implementation">Github Implementation</h4> <p><a href="https://github.com/beabevi/ESAN">https://github.com/beabevi/ESAN</a>.</p> <h3 id="references">References</h3> <p>[1] Bevilacqua, Beatrice, et al. “Equivariant subgraph aggregation networks.” ICLR 2022.</p> <p>[2] Maron, Haggai, et al. “On learning sets of symmetric elements.” International Conference on Machine Learning. PMLR, 2020.</p> <p>[3] Zaheer, Manzil, et al. “Deep sets.” Advances in neural information processing systems 30 (2017).</p> <p>[4] Qi, Charles R., et al. “Pointnet: Deep learning on point sets for 3d classification and segmentation.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A review on the paper "Equivariant Subgraph Aggregation Networks" (ICLR 2022)]]></summary></entry></feed>