<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Equivariant Subgraph Aggregation Networks | Minsu Kim</title> <meta name="author" content="Minsu Kim"/> <meta name="description" content="A review on the paper "Equivariant Subgraph Aggregation Networks" (ICLR 2022)"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/diffeqml.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://minsuukim.github.io/2022/04/16/esan.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Minsu </span>Kim</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/collaborators/">collaborators</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/workshop%20presentations/">workshop presentations</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Equivariant Subgraph Aggregation Networks</h1> <p class="post-meta">April 16, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> </p> </header> <article class="post-content"> <h1 id="equivariant-subgraph-aggregation-networks">Equivariant Subgraph Aggregation Networks</h1> <p>We will present a blog post on <a href="https://arxiv.org/abs/2110.02910" target="_blank" rel="noopener noreferrer"><em>“Equivariant Subgraph Aggregation Networks”</em></a> from Beatrice Bevilacqua et al. [1], which has been accepted as a Spotlight presentation in <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR 2022</a>. This was originally a course review for <a href="https://dsail.gitbook.io/isyse-review/paper-review/2022-spring-paper-review" target="_blank" rel="noopener noreferrer">KSE 527</a> at KAIST.</p> <h2 id="1-problem-definition"><strong>1. Problem Definition</strong></h2> <h3 id="message-passing-neural-networks-and-their-drawbacks">Message Passing Neural Networks and their Drawbacks</h3> <p>The commonly used Message-passing Graph Neural Networks (MPNNs) consist of several layers which perform node-wise aggregation of information from neighbour nodes.</p> <p>There are two main important characteristics which make these graph neural networks appealing:</p> <ul> <li> <strong>Locality</strong>: computations require only the immediate neighbours of a node. Unlike Convolutional Neural Networks (CNNs), which require structured data, MPNNs are able to capture complex relationships on unstructured graphs, which makes them suitable for a number of different tasks that cannot be solved by the classical convolutions of CNNs.</li> <li> <strong>Linear complexity</strong>: MPNNs benefit from linear in the number of edges: this means that they can be easily <em>scalable</em> to large numbers of nodes and edges. Roughly speaking, this <em>linear complexity</em> property means that doubling the number of nodes in our graph will double the requirement in computational resources.</li> </ul> <p>However, MPNNs suffer from a limitation on their expressive power. In particular, it has been demonstrated that they are at most as expressive as the <a href="https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49" target="_blank" rel="noopener noreferrer">Weisfeiler-Lehman graph isomorphism test</a>, a classical method that can be used to determine if the structure of two graphs is <em>equivalent</em> (i.d. <em>iso</em>: same, <em>morphic</em>: shape). So, what is this Weisfeiler-Lehman test?</p> <h3 id="a-brief-introduction-to-the-weisfeiler-lehman-test">A Brief Introduction to the Weisfeiler-Lehman Test</h3> <p>The Weisfeiler-Lehman (WL) graph isomorphism test is a <em>necessary</em> but <em>insufficient</em> condition to determine if two graphs have the same structure. In particular the WL-test is an efficient <em>heuristics</em> that can tell in polynomial time if two graphs are not isomorphic:</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/wl_test-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/wl_test-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/wl_test-1400.webp"></source> <img src="/assets/img/blogs/esan/wl_test.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>The algorithm works by <em>colouring</em> the graph nodes (i.d. assigning labels) and keeps assigning these colors by aggregating neighbors (we note that this aggregation is indeed a form of message passing!) and stops when the coloring is stable. When this happens, there are two possibilities:</p> <ul> <li>The colors are <strong>different</strong>: the two graphs are <strong>not</strong> isomorphic</li> <li>The colors are <strong>same</strong>: the two graphs <strong>may be</strong> (we are not sure!) isomorphic</li> </ul> <p>This is the reason why it is an <em>insufficient</em> condition for isomorphism. Let’s look at this example:</p> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/isomorphic_graphs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/isomorphic_graphs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/isomorphic_graphs-1400.webp"></source> <img src="/assets/img/blogs/esan/isomorphic_graphs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>In this case, both of the graphs have the same number of nodes and edges and they are the same for the WL-test (same colors). However, they are clearly <strong>not isomorphic</strong>! This kind of structure occurs frequently in many graphs, such as molecular bonds.</p> <h3 id="extension-to-k-wl">Extension to <em>k</em>-WL</h3> <p>It has been shown that we can extend the WL-test to a higher order (<em>k</em>) version, namely the <em>k</em>-WL test. Except for 2-WL, it can be shown that (<em>k + 1</em>)-WL is strictly stronger than <em>k</em>-WL. However, there is no version for infinite dimensional WL, i.d. having a <em>general</em> and most importantly <em>sufficient</em> condition for isomorphism for <em>any</em> graph.</p> <h2 id="2-motivation"><strong>2. Motivation</strong></h2> <p>We have seen how MPNNs suffer from an <em>expressive power</em> drawback. While a number of approaches have been introduced in the literature to have more expressive GNNs, these have limitations such as:</p> <ul> <li>Poor generalization</li> <li>Computationally expensive</li> <li>Require deep domain knowledge</li> </ul> <p>What if we had a way a <em>general, inexpensive and domain-agnostic</em> way of performing augmenting GNN expressivity?</p> <h3 id="using-subgraphs-as-wl-tests">Using Subgraphs as WL-tests</h3> <p>A core idea of the paper is to divide a graph into subgraphs. We can use some <em>selection policies</em> (described later in the post) to obtain a <em>bag of subgraphs</em> from the original graph. Let’s see this example:</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/teaser-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/teaser-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/teaser-1400.webp"></source> <img src="/assets/img/blogs/esan/teaser.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>As we can see on the left, there are WL-indistinguishable subgraphs: they are <strong>not-isomorphic</strong>, but they yield the <strong>same coloring</strong>. Now, if we split them into subgraphs (on the right) and run again the WL-test, we will notice that the produce coloring are indeed <strong>different</strong>: this implies that they are actually <strong><em>not isomorphic</em></strong>!</p> <h2 id="3-method"><strong>3. Method</strong></h2> <p>We will introduce the framework devised by the authors, <em>ESAN</em> (Equivariant Subgraph Aggregation Networks). The main idea behind ESAN is to represent the graph \(G\) as a <em>bag</em> (i.d. a multiset):</p> \[S_G=\{\{ G_1,\dots,G_m \}\}\] <p>of its subgraphs. Then, we can make predictions on a graph based on this subset: \(F(G):=F(S_G)\)</p> <p>There are two essential points to consider then: Two crucial questions pertain to this approach: (1) , and (2)</p> <ol> <li>How to define \(F(S_G)\), i.e. architecture should we use to process bags of graphs?</li> <li>How do we select \(S_G\), the graph subgraph selection policy?</li> </ol> <h3 id="bag-of-graphs-encoder-architecture">Bag-of-Graphs Encoder Architecture</h3> <h4 id="symmetry-groups">Symmetry groups</h4> <p>First of all, let’s start by describing how to represent a set of graphs (the so-called <em>bag-of-graphs</em>). We can summarize it in the following image:</p> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/symmetries-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/symmetries-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/symmetries-1400.webp"></source> <img src="/assets/img/blogs/esan/symmetries.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>The graph is split into a bag of subgraphs where $\tau$ permutes the subgraphs in the set and $\sigma$ permutes the nodes in the subgraphs. In other words, this representation creates a <em>symmetry group</em>: by requiring the representation to permute the subgraphs and their nodes, we can obtain a neural architecture which is <strong>equivariant</strong> to this group.</p> <p>Equivariant means that no matter the order of transformations, the output will be always the same (on the left):</p> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/equivariance-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/equivariance-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/equivariance-1400.webp"></source> <img src="/assets/img/blogs/esan/equivariance.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>which is a desirable property in the realm of Graph Neural Networks - in the example above, it is easy to see how this is desirable for a classification CNN.</p> <h4 id="architecture-overview">Architecture Overview</h4> <p>Now, we can formulate the architecture of <strong>DSS-GNN</strong>:</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/arch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/arch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/arch-1400.webp"></source> <img src="/assets/img/blogs/esan/arch.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>where DSS-GNN stands for the somewhat-lenghty “Deep Sets of Symmetric Objects - Graph Neural Network”; the symmetric objects are the bags of subgraphs that we obtained before. This architecture is composed of three main layers:</p> \[F_{\text{DSS-GNN}} = E_{\text{sets}}\circ R_{\text{subgraphs}} \circ E_{\text{subgraphs}}\] <p>Let’s decompose them one by one!</p> <ol> <li> <p><strong>Equivariant Feature Encoder</strong>: \(E_{\text{subgraphs}}\) is composed of several \(H\)-equivariant layers. Its purpose is to learn useful node features for all the nodes in all subgraphs. Each $H$-equivariant layer (on the right) processes bags of subgraphs accounting for their natural symmetry and it is composed by the following: \((L(\mathcal{A},\mathcal{X}))_i= L^1(A_i,X_i) + L^2\left(\textstyle\sum _{j=1}^m A_j,\textstyle\sum _{j=1}^m X_j\right)\) where, \(A_j, X_j\) are the adjacency and feature matrices of the \(j\)-th subgraph and \((L(\mathcal{A},\mathcal{X}))_i\) is the output of the layer on the $i$-th subgraph. So what are these \(L_1\) and \(L_2?\) These can be any type of GNN layer. While \(L_1\) encodes each subgraph <em>separately</em> , \(L_2\) <em>aggregates</em> information among the subgraphs (which is called <em>information sharing</em>). This is also common with the seminal DSS paper [2].</p> </li> <li> <p><strong>Subgraph Readout Layer</strong>: \(R_{\text{subgraphs}}\) given the output from the first step, this generates an invariant feature vector for each subgraph independently by aggregating the graph node and/or edge data. The modalities can change, but we can for instance aggregate with a mean operator.</p> </li> <li> <p><strong>Set Encoder</strong>: \(E_{\text{sets}}\) is is a universal set encoder, such as DeepSets [3] or PointNets [4]. This part aggregates the set of preprocessed subgraphs with <em>invariant-per-subgraph</em> features, so that we can obtain the final graph representation.</p> </li> </ol> <h3 id="subgraph-selection-policies">Subgraph Selection Policies</h3> <p>Selecting the subgraphs is of paramount importance: this determines how <em>expressive</em> our representation will be! The authors study four main policies:</p> <ol> <li> <strong>Node-deleted policy</strong>: a graph is mapped to the set containing all subgraphs that can be obtained from the original graph by removing a single node</li> <li> <strong>Edge-deleted policy</strong>: similar to above, but it is defined as the set of all subgraphs we obtain by removing a single edge.</li> <li> <strong>EGO-networks policy</strong>: this policy maps each graph to a set of ego-networks of some specified depth, one for each node in the graph. An ego-network is, simply put, the network obtained by looking at the neighboorhood of a node only (where <em>ego</em> means “I”, meaning we focus on the perspective of one node). A \(k\)-Ego-network of a node is its \(k\)-hop neighbourhood with the induced connectivity).</li> <li> <strong>EGO+ policy</strong>: We can also consider a variant of the ego-networks policy where the root node (a is <em>e</em> in the example below) holds an identifying feature (EGO+).</li> </ol> <div style="width:80%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/ego-network-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/ego-network-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/ego-network-1400.webp"></source> <img src="/assets/img/blogs/esan/ego-network.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>However, obtaining the set of subgraphs from a large network can be overly expensive. That is why the authors propose a <strong>stochastic subsampling</strong> in which only a small subset of subgraphs is sampled to calculate the loss function.</p> <h3 id="theoretical-contributions">Theoretical Contributions</h3> <p>An important contribution of the authors is showing that their model ESAN is expressive by demonstrating it theoretically.</p> <p>In particular, the authors devise a new variant of the Weisfeiler-Lehman test dubbed “DSS-WL” with different architectural and subgraph selection policies. To sum up, an important theoretical conclusion is that the ESAN architecture can distinguish 3-WL equivalent graphs using only a WL graph encoder - which is, the standard MPNN. Moreover, this can enhance the expressive power of stronger architectures.</p> <h2 id="4-experiments"><strong>4. Experiments</strong></h2> <p>The experiments demonstrate strong performance of the DSS-GNN model. The authors also compare the case in which the <em>information sharing</em> layer \(L_2\) is set to \(0\). Base encoders and graph selection policies are reported in parentheses.</p> <h3 id="demonstrating-expressive-power-on-synthetic-datasets">Demonstrating Expressive Power on Synthetic Datasets</h3> <p>The authors use <em>EXP, CEXP</em> and <em>CS</em> which are designed so that any 1-WL GNN cannot do better than random guess, such as a vanilla MPNN.</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/synthetic-datasets-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/synthetic-datasets-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/synthetic-datasets-1400.webp"></source> <img src="/assets/img/blogs/esan/synthetic-datasets.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>We notice how the variants of baselines with DSS-GNN manage to - perfectly - solve the tasks while 1-WL GNN basically perform random guesses.</p> <h3 id="tudatasets">TUDatasets</h3> <p>This is a benchmark set of real datasets for classification and regression of various type, such as user molecules (PROTEINS) and movies (IMDB).</p> <div style="width:100%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/tu_datasets-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/tu_datasets-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/tu_datasets-1400.webp"></source> <img src="/assets/img/blogs/esan/tu_datasets.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>Results show that the approach achieves state of the art in one dataset (PTC), but it still retains comparable results to the SoTA (state-of-the-art).</p> <h3 id="ogb">OGB</h3> <p>These datasets comprise <em>OGBG-MOLHIV</em> and <em>OGBG-MOLTOX21</em>, which are dedicated to molecular property predictions.</p> <div style="width:60%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/ogb-dataset-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/ogb-dataset-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/ogb-dataset-1400.webp"></source> <img src="/assets/img/blogs/esan/ogb-dataset.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>These results shows how ESAN achieve SoTA in both datasets with basically all of the selection policies.</p> <h3 id="zinc12k">Zinc12k</h3> <p><em>ZINC12K</em> is a larger scale molecular benchmark: here, the authors want to demonstrate how their method can scale up to much larger dimensions:</p> <div style="width:60%; margin:0 auto; text-align: center;"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blogs/esan/zink12k-dataset-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blogs/esan/zink12k-dataset-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/esan/zink12k-dataset-1400.webp"></source> <img src="/assets/img/blogs/esan/zink12k-dataset.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>As the table shows, ESAN does not achieve SoTA. <em>However</em>, it achieves SoTA among the <strong>domain-agnostic</strong> models: indeed, SoTA is achieved by a model employing strong inductive biases, which is somewhat an “unfair” comparison since the authors employ <em>ad-hoc</em> tricks to solve the problem. This shows how ESAN can expand on large scales by being both <em>powerful</em> and <em>domain-agnostic</em>.</p> <h2 id="5-conclusion"><strong>5. Conclusion</strong></h2> <p>We have reviewed <em>ESAN</em>: Equivariant Subgraph Aggregration Networks, a novel graph paradigm. The core idea of ESAN is to decompose a graph into bags of subgraphs via subgraph selection policies and process them according to symmetry groups. A further contribution is that the proposed model is provably more powerful than standard MPNNs in performing WL graph isomorphism tests. Experimental results reach state-of-the-art in certain tasks and, where they don’t, still remain very competitive by achieving close results while being domain-agnostic.</p> <h4 id="limitations">Limitations</h4> <p>The proposed model needs to select the bags of subgraphs and by processing a large amount of obtained subgraphs it is strictly more computationally expensive than standard MPNNs, which limit their applicability, although the authors include a subsampling policy to partly overcome this issue. A further limitation is that, while it is true that the authors provide mathematical proofs and code, their approach still does not achieve SoTA in many of the TUDatasets, a standard graph benchmarking problem, highlighting the fact that further studies should be done to prove that the proposed ESAN approach is strictly superior to competitors not only theoretically, but also <em>experimentally</em>.</p> <hr> <h2 id="author-information"><strong>Author Information</strong></h2> <p><strong>Federico Berto</strong> <a href="https://fedebotu.github.io/" target="_blank" rel="noopener noreferrer">Personal Website</a></p> <p>Affiliation: KAIST, Industrial &amp; Systems Engineering Department</p> <p>MSc students at <a href="http://silab.kaist.ac.kr/" target="_blank" rel="noopener noreferrer">SILAB</a></p> <p>Member of the open research group <a href="https://github.com/DiffEqML" target="_blank" rel="noopener noreferrer">DiffEqML</a></p> <h2 id="6-reference--additional-materials"><strong>6. Reference &amp; Additional materials</strong></h2> <h4 id="github-implementation">Github Implementation</h4> <p><a href="https://github.com/beabevi/ESAN" target="_blank" rel="noopener noreferrer">https://github.com/beabevi/ESAN</a>.</p> <h3 id="references">References</h3> <p>[1] Bevilacqua, Beatrice, et al. “Equivariant subgraph aggregation networks.” ICLR 2022.</p> <p>[2] Maron, Haggai, et al. “On learning sets of symmetric elements.” International Conference on Machine Learning. PMLR, 2020.</p> <p>[3] Zaheer, Manzil, et al. “Deep sets.” Advances in neural information processing systems 30 (2017).</p> <p>[4] Qi, Charles R., et al. “Pointnet: Deep learning on point sets for 3d classification and segmentation.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2022 Minsu Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. Last updated: October 09, 2022. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>